{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\")\n",
    "print(file.read())\n",
    "search_word = \"Anton Paar RheoCompass\"\n",
    "if(search_word == file):\n",
    "    print(\"word found\")\n",
    "else:\n",
    "    print(\"word not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdftableextract as pdf\n",
    "\n",
    "pages = [\"2\"]\n",
    "cells = [pdf.process_page(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669695.pdf\",p) for p in pages]\n",
    "\n",
    "#flatten the cells structure\n",
    "cells = [item for sublist in cells for item in sublist ]\n",
    "\n",
    "#without any options, process_page picks up a blank table at the top of the page.\n",
    "#so choose table '1'\n",
    "li = pdf.table_to_list(cells, pages)[1]\n",
    "\n",
    "#li is a list of lists, the first line is the header, last is the footer (for this table only!)\n",
    "#column '0' contains store names\n",
    "#row '1' contains column headings\n",
    "#data is row '2' through '-1'\n",
    "\n",
    "data =pd.DataFrame(li[2:-1], columns=li[1], index=[l[0] for l in li[2:-1]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "tables = camelot.read_pdf(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\")\n",
    "tables.export(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.xlsx\", f='excel', compress=True) #json, excel, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "mypdf = open(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\", mode='rb')\n",
    "print(mypdf)\n",
    "\n",
    "pdf_document = PyPDF2.PdfFileReader(mypdf)\n",
    "\n",
    "pdf_document.numPages\n",
    "\n",
    "first_page = pdf_document.getPage(0)\n",
    "\n",
    "print(first_page.extractText())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a for-loop to open many files -- leave a comment if you'd #like to learn how\n",
    "filename = \"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\"\n",
    "#open allows you to read the file\n",
    "pdfFileObj = open(filename,'rb')\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "#The while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "#This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "if text != \"\":\n",
    "   text = text\n",
    "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "else:\n",
    "   text = textract.process(filename, method='tesseract', language='eng')\n",
    "# Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "# Now, we will clean our text variable, and return it as a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The word_tokenize() function will break our text phrases into #individual words\n",
    "tokens = word_tokenize(text)\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# This function will extract and return the pdf file text content.\n",
    "def extractPdfText(filePath=\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\"):\n",
    "\n",
    "    # Open the pdf file in read binary mode.\n",
    "    fileObject = open(filePath, 'rb')\n",
    "\n",
    "    # Create a pdf reader .\n",
    "    pdfFileReader = PyPDF2.PdfFileReader(fileObject)\n",
    "\n",
    "    # Get total pdf page number.\n",
    "    totalPageNumber = pdfFileReader.numPages\n",
    "\n",
    "    # Print pdf total page number.\n",
    "    print('This pdf file contains totally ' + str(totalPageNumber) + ' pages.')\n",
    "\n",
    "    currentPageNumber = 1\n",
    "    text = \"Anton Paar RheoCompass\"\n",
    "\n",
    "    # Loop in all the pdf pages.\n",
    "    while(currentPageNumber < totalPageNumber ):\n",
    "\n",
    "        # Get the specified pdf page object.\n",
    "        pdfPage = pdfFileReader.getPage(currentPageNumber)\n",
    "\n",
    "        # Get pdf page text.\n",
    "        text = text + pdfPage.extractText()\n",
    "\n",
    "        # Process next page.\n",
    "        currentPageNumber += 1\n",
    "\n",
    "    if(text == \"Anton Paar RheoCompass\"):\n",
    "        # If can not extract text then use ocr lib to extract the scanned pdf file.\n",
    "        text = textract.process(filePath, method='tesseract', encoding='utf-8')\n",
    "       \n",
    "    return text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "filename = \"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\"\n",
    "\n",
    "pdfFileObj = open(filename,'rb')\n",
    " \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    " \n",
    "num_of_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text =\"Anton Paar RheoCompass\"\n",
    " \n",
    "while count < num_of_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "  \n",
    "# creating a pdf file object \n",
    "pdfFileObj = open( \"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\", 'rb') \n",
    "  \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "  \n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages) \n",
    "  \n",
    "# creating a page object \n",
    "pageObj = pdfReader.getPage(0) \n",
    "  \n",
    "# extracting text from page \n",
    "print(pageObj.extractText()) \n",
    "  \n",
    "# closing the pdf file object \n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This function will remove all stop words and punctuations in the text and return a list of keywords.\n",
    "def extractKeywords(text):\n",
    "    # Split the text words into tokens\n",
    "    wordTokens = word_tokenize(text)\n",
    "\n",
    "    # Remove blow punctuation in the list.\n",
    "    punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "    # Get all stop words in english.\n",
    "    stopWords = stopwords.words('english')\n",
    "\n",
    "    # Below list comprehension will return only keywords tha are not in stop words and  punctuations\n",
    "    keywords = [word for word in wordTokens if not word in stopWords and not word in punctuations]\n",
    "   \n",
    "    return keywords\n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    pdfFilePath = \"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\"\n",
    "   \n",
    "    pdfText = extractPdfText(pdfFilePath)\n",
    "    print('There are ' + str(pdfText.__len__()) + ' word in the pdf file.')\n",
    "    #print(pdfText)\n",
    "\n",
    "    keywords = extractKeywords(pdfText)\n",
    "    print('There are ' + str(keywords.__len__()) + ' keyword in the pdf file.')\n",
    "    #print(keywords)'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdfFileObj = open(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669857.pdf\", 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict=False)\n",
    "search_word = \"Anton Paar RheoCompass\"\n",
    "search_word_count = 0\n",
    "for pageNum in range(1, pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(pageNum)\n",
    "    text = pageObj.extractText().encode('utf-8')\n",
    "    search_text = text.lower().split()\n",
    "    for word in search_text:\n",
    "        if search_word in word.decode(\"utf-8\"):\n",
    "            search_word_count += 1\n",
    "        \n",
    "print(\"The word {} was found {} 11669857\".format(search_word, search_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def searchInPDF(filename, key):\n",
    "    occurrences = 0\n",
    "    pdfFileObj = open(filename,'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "    if text != \"\":\n",
    "       text = text\n",
    "    else:\n",
    "       text = textract.process(filename)\n",
    "    tokens = word_tokenize(text)\n",
    "    punctuation = ['(',')',';',':','[',']',',']\n",
    "    stop_words = stopwords.words('english')\n",
    "    keywords = [word for word in tokens if not word in stop_words and  not word in punctuation]\n",
    "    for k in keywords:\n",
    "        if key == k: occurrences+=1\n",
    "    return occurrences \n",
    "\n",
    "pdf_filename = \"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669695.pdf\"\n",
    "search_for = 'Anton Paar RheoCompass'\n",
    "print (searchInPDF (pdf_filename,search_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# open the pdf file\n",
    "object = PyPDF2.PdfFileReader(\"C:\\\\Users\\\\Drl\\\\Desktop\\\\pdf table extract\\\\11669695.pdf\")\n",
    "\n",
    "# get number of pages\n",
    "NumPages = object.getNumPages()\n",
    "\n",
    "# define keyterms\n",
    "String = \"Anton Paar RheoCompass\"\n",
    "\n",
    "# extract text and do the search\n",
    "for i in range(0, NumPages):\n",
    "    PageObj = object.getPage(i)\n",
    "    print(\"this is page \" + str(i)) \n",
    "    Text = PageObj.extractText() \n",
    "    # print(Text)\n",
    "    ResSearch = re.search(String, Text)\n",
    "    print(ResSearch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
